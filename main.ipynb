{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recurrent Neural Network (RNN) from Scratch in NumPy for Text Generation\n",
    "\n",
    "**Author** Niklas Wicklund\n",
    "\n",
    "**Last revised** 07-04-2024\n",
    "\n",
    "\n",
    "In this project, I delve into the realm of natural language processing (NLP) by constructing a Recurrent Neural Network (RNN) entirely from scratch using NumPy. My goal is to train this RNN on a given text dataset (in this case, a sample from Harry Potter) and then use it to generate new text that mimics the style and structure of the original text\n",
    "\n",
    "#### Why from scratch?\n",
    "\n",
    "While there are powerful deep learning libraries available like TensorFlow and PyTorch, understanding the mechanics of an RNN at this level can be immensely beneficial. Building an RNN from scratch offers a deep understanding of its inner workings. By implementing the core functionalities myself, I gain insights into the fundamental concepts of sequence modeling, backpropagation, and gradient descent optimization. \n",
    "\n",
    "_This project was done as an assignment in one of my courses at KTH Royal Institute of Technology_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book():\n",
    "    # Read all characters in the txt file goblet_book.txt and store in a python list, we remove all newlines and tabs\n",
    "    with open('goblet_book.txt', 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Create a list of all unique characters in the text\n",
    "    book_chars = list(set(data))\n",
    "    K = len(book_chars)\n",
    "    # Create a dictionary that maps each character to a unique index\n",
    "    char_to_ind = { char:ind for ind, char in enumerate(book_chars) }\n",
    "    # Create a dictionary that maps each unique index back to a character\n",
    "    ind_to_char = { ind:char for ind, char in enumerate(book_chars) }\n",
    "    return [*data], book_chars, char_to_ind, ind_to_char, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, K):\n",
    "    # using np.eye\n",
    "    return np.eye(K)[x].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, m, K,char_to_ind,ind_to_char):\n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        \n",
    "        self.char_to_ind = char_to_ind\n",
    "        self.ind_to_char = ind_to_char\n",
    "\n",
    "        # Initialize the parameters\n",
    "        self.initalize_parameters(m, K)\n",
    "\n",
    "    def initalize_parameters(self, m, K):\n",
    "        \"\"\"\n",
    "        Initialize the parameters of the RNN model.\n",
    "        Inputs:\n",
    "            m: Number of hidden units\n",
    "            K: Number of unique characters in the text\n",
    "        \"\"\"\n",
    "\n",
    "        sig = 0.05\n",
    "        # Initialize the bias parameters and store in a dictionary\n",
    "        self.params = {}\n",
    "        self.params['b'] = np.zeros((m, 1))\n",
    "        self.params['c'] = np.zeros((K, 1))\n",
    "        # Initialize the weight parameters and store in a dictionary as random numbers\n",
    "        self.params['U'] = np.random.randn(m, K) * sig\n",
    "        self.params['W'] = np.random.randn(m, m) * sig\n",
    "        self.params['V'] = np.random.randn(K, m) * sig\n",
    "    \n",
    "    def get_book_sequence(self,book_data,seq_length,e):\n",
    "        \"\"\"\n",
    "        Generate a sequence of characters from the book data.\n",
    "        Inputs:\n",
    "            book_data: List of characters in the book\n",
    "            seq_length: Length of the sequence\n",
    "            e: Index of the sequence\n",
    "        Outputs:\n",
    "            X: Sequence of input vectors (one hot encoded) (K x seq_length)\n",
    "            Y: Sequence of output vectors (one hot encoded) (K x seq_length)\n",
    "        \"\"\"\n",
    "        X_chars = book_data[e:seq_length + e]\n",
    "        Y_chars = book_data[e+1:seq_length+e+1]\n",
    "            \n",
    "\n",
    "        # Convert the labelled sequence to one hot encoded vectors with shape (K x seq_length)\n",
    "        X = np.eye(self.K,dtype=int)[[self.char_to_ind[char] for char in X_chars]].T\n",
    "        Y = np.eye(self.K,dtype=int)[[self.char_to_ind[char] for char in Y_chars]].T\n",
    "        return X,Y\n",
    "\n",
    "    def softmax(self, o):\n",
    "        eps = np.finfo(float).eps\n",
    "        p = np.exp(o) / (np.sum(np.exp(o)) + eps)\n",
    "        return p\n",
    "    \n",
    "    def l_cross(self, Y, p):\n",
    "        \"\"\"\n",
    "        Compute the cross entropy loss between the true labels Y and the predicted probabilities P.\n",
    "        Inputs:\n",
    "            Y: True labels (K x n)\n",
    "            P: Predicted probabilities (K x n)\n",
    "        Outputs:\n",
    "            loss: Cross entropy loss\n",
    "        \"\"\"\n",
    "        eps = np.finfo(float).eps\n",
    "        loss = -np.sum(Y*np.log(p + eps))\n",
    "        return loss\n",
    "    def compute_loss(self, X,Y,h0):\n",
    "       # Forward pass\n",
    "        P,_ = self.forward_pass(h0,X,Y)\n",
    "        # Compute the loss\n",
    "        loss = self.l_cross(Y,P)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def compute_gradients(self, X, Y, P):\n",
    "        \"\"\"\n",
    "        Perform a backward pass of the RNN model.\n",
    "        Inputs:\n",
    "            X: Sequence of input vectors (one hot encoded) (K x n)\n",
    "            Y: Sequence of output vectors (one hot encoded) (K x n)\n",
    "            P: Softmax of the output vectors (K x n)\n",
    "        Outputs:\n",
    "            gradients: Dictionary containing the gradients of the parameters\n",
    "        \"\"\"\n",
    "        n = X.shape[1]\n",
    "        grads = {}\n",
    "        grads['dU'] = np.zeros((self.m, self.K))\n",
    "        grads['dW'] = np.zeros((self.m, self.m))\n",
    "        grads['dV'] = np.zeros((self.K, self.m))\n",
    "        grads['db'] = np.zeros((self.m, 1))\n",
    "        grads['dc'] = np.zeros((self.K, 1))\n",
    "\n",
    "        do = [None]*n\n",
    "        dh = [None]*n\n",
    "        da = [None]*n\n",
    "\n",
    "        # The last layer is different and is done before the loop\n",
    "        do[-1] = (-(Y[:,-1] - P[:,-1]).T).reshape(-1,1)\n",
    "        dh[-1] = (self.params['V'].T @ do[-1]).reshape(-1,1)\n",
    "        c = (1 - np.tanh(self.forward['a'][-1])**2).reshape(-1)\n",
    "        a = np.diag(c)\n",
    "        b = dh[-1]\n",
    "\n",
    "        # a is shape (m, m), b is shape (m, 1)\n",
    "        da[-1] = (a @ b).reshape(-1,1)\n",
    "        \n",
    "        # Compute the gradients of h and a\n",
    "        for t in reversed(range(0,n)):\n",
    "            do[t] = (-(Y[:,t] - P[:,t]).T).reshape(-1,1)\n",
    "            assert(do[t].shape == (self.K, 1))\n",
    "\n",
    "            grads['dV'] += do[t] @ self.forward['h'][t].T\n",
    "            grads['dc'] += do[t]\n",
    "            \n",
    "            # Compute the gradients of h and a\n",
    "            if t == n-1:\n",
    "                dh[t] = self.params['V'].T @ do[t]\n",
    "            else:\n",
    "                dh[t] = self.params['V'].T @ do[t] + self.params['W'].T @ da[t+1]\n",
    "\n",
    "            da[t] = np.multiply(dh[t], (1 - np.tanh(self.forward['a'][t])**2).reshape(-1,1))\n",
    "\n",
    "            # Compute the gradients of U and W\n",
    "            grads['dU'] += np.matmul(da[t], X[:,t].reshape(-1,1).T)\n",
    "            grads['dW'] += np.matmul(da[t], self.forward['h'][t-1].T)\n",
    "\n",
    "            grads['db'] += da[t]\n",
    "            \n",
    "\n",
    "        # Clip the gradients, to prevent exploding gradients\n",
    "        for grad in grads:\n",
    "            grads[grad] = np.clip(grads[grad], -5, 5)\n",
    "        return grads\n",
    "\n",
    "    def forward_step(self, h_prev, X):\n",
    "        \"\"\"\n",
    "        Perform a forward step of the RNN model.\n",
    "        Inputs:\n",
    "            h_prev: Previous hidden state (m x 1)\n",
    "            X: Input vector (one hot encoded) (K x 1)\n",
    "        Outputs:\n",
    "            a: Activation (m x 1)\n",
    "            h: Hidden state (m x 1)\n",
    "            o: Output (K x 1)\n",
    "            p: Softmax of the output (K x 1)\n",
    "        \"\"\"\n",
    "        a = self.params['W'] @ h_prev + (self.params['U']@X).reshape(self.m,1) + self.params['b']\n",
    "        h = np.tanh(a)\n",
    "        o = self.params['V'] @ h + self.params['c']\n",
    "        p = self.softmax(o)\n",
    "\n",
    "        # Reshape p to be a column vector (K x 1)\n",
    "        return a, h, o, p.reshape(-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward_pass(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the RNN model.\n",
    "        Inputs:\n",
    "            h0: Initial hidden state (m x 1)\n",
    "            X: Sequence of input vectors (one hot encoded) (K x n)\n",
    "            Y: Sequence of output vectors (one hot encoded) (K x n)\n",
    "        Outputs:\n",
    "            P: Softmax of the output vectors (K x n)\n",
    "            forward: Dictionary containing the intermediate values from the forward pass\n",
    "        \"\"\"\n",
    "        n = X.shape[1]\n",
    "        forward = {}\n",
    "        forward['h'] = [None]*(n+1)\n",
    "        forward['a'] = [None]*n\n",
    "        forward['o'] = [None]*n\n",
    "        forward['p'] = [None]*n\n",
    "\n",
    "        h_prev= h0\n",
    "        for t in range(n):\n",
    "            forward['a'][t], forward['h'][t], forward['o'][t], forward['p'][t] = self.forward_step(h_prev, X[:,t])\n",
    "            h_prev = forward['h'][t]\n",
    "        forward['h'][-1] = h0\n",
    "\n",
    "        P = np.array(forward['p']).T\n",
    "        \n",
    "        forward['loss'] = self.l_cross(Y,P)\n",
    "        self.forward = forward\n",
    "        return P, forward\n",
    "\n",
    "    def train(self,book_data, h0, eta, n_epochs, seq_length,optimizer = 'adagrad'):\n",
    "        \"\"\"\n",
    "        Train the RNN model using the book data.\n",
    "        Inputs:\n",
    "            book_data: List of characters in the book\n",
    "            h0: Initial hidden state\n",
    "            eta: Learning rate\n",
    "            n_epochs: Number of epochs\n",
    "            seq_length: Length of the sequence\n",
    "        Outputs:\n",
    "            params: Dictionary containing the trained parameters\n",
    "            smooth_loss: List of the smoothed loss values\n",
    "            best_model: Dictionary containing the best model parameters\n",
    "            df: Dataframe containing the relevant data during training\n",
    "        \"\"\"\n",
    "        with open('stats.txt', 'a') as f:\n",
    "            #Print the current training settings to the file\n",
    "            f.write(\"\\n\\n### New training session ###\\n\\n\")\n",
    "            f.write(f\"K: {self.K}, Length of book: {len(book_data)}\\n\")\n",
    "            f.write(f\"eta: {eta}, n_epochs: {n_epochs}, seq_length: {seq_length}\\n\")\n",
    "        \n",
    "        ada_params = {\n",
    "            'U': np.zeros_like(self.params['U']),\n",
    "            'W': np.zeros_like(self.params['W']),\n",
    "            'V': np.zeros_like(self.params['V']),\n",
    "            'b': np.zeros_like(self.params['b']),\n",
    "            'c': np.zeros_like(self.params['c'])\n",
    "        }\n",
    "        eps = np.finfo(float).eps\n",
    "\n",
    "        hprev = np.zeros((self.m, 1))\n",
    "\n",
    "        # Make use of smooth_loss to compute the loss\n",
    "        smooth_loss = None\n",
    "        smooth_loss_epoch = []\n",
    "\n",
    "        #Define best dictionary to store the best parameters, from start have same parameters as self.params but copies\n",
    "        best_model = {\n",
    "            'loss': np.inf,\n",
    "            'params':{}\n",
    "        }\n",
    "        for key in self.params:\n",
    "            best_model['params'][key] = self.params[key].copy()\n",
    "\n",
    "        #Define a dataframe that stores relevant data during training\n",
    "        df = pd.DataFrame(columns=['epoch','update_step','e', 'smooth_loss','loss'])\n",
    "        update_step = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            hprev = np.zeros((self.m, 1))\n",
    "            # For each epoch we should iterate the entire book\n",
    "            smooth_loss_list = []\n",
    "            # for e loop that iterates over the book\n",
    "            with trange(0, len(book_data), seq_length) as t:\n",
    "                for e in t:\n",
    "                    # Check if e is too large\n",
    "                    if e + seq_length + 1 >= len(book_data):\n",
    "                        break\n",
    "\n",
    "                    X,Y = self.get_book_sequence(book_data, seq_length,e)\n",
    "                    p, forward = self.forward_pass(hprev, X, Y)\n",
    "\n",
    "                    # Save last hidden state to hprev (remeber that h is 1 longer so the last is the initial hidden state, need to save the second last)\n",
    "                    hprev = forward['h'][-2]\n",
    "                    loss = forward['loss']\n",
    "                    if(smooth_loss == None):\n",
    "                        smooth_loss = loss\n",
    "                    else:\n",
    "                        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "                    if loss < best_model['loss']:\n",
    "                        for key in self.params:\n",
    "                            best_model['params'][key] = self.params[key].copy()\n",
    "                        best_model['loss'] = loss\n",
    "                    \n",
    "                    smooth_loss_list.append(smooth_loss)\n",
    "                    gradients = self.compute_gradients(X, Y, p)\n",
    "\n",
    "                    if optimizer == 'adagrad':\n",
    "                        for key in self.params:\n",
    "                            # Adaboost\n",
    "                            ada_params[key] += gradients['d'+key]**2\n",
    "                            self.params[key] -= (eta*gradients['d'+key])/(np.sqrt(ada_params[key]) + eps)\n",
    "                    else:\n",
    "                        # Test to use Adam instead\n",
    "                        beta1 = 0.9\n",
    "\n",
    "                        for key in self.params:\n",
    "                            ada_params[key] = beta1*ada_params[key] + (1-beta1)*gradients['d'+key]**2\n",
    "                            self.params[key] -= (eta*gradients['d'+key])/(np.sqrt(ada_params[key]) + eps)\n",
    "\n",
    "                    if(update_step % 100 == 0):\n",
    "                        t.set_description(f'Smooth loss <{smooth_loss}>')\n",
    "                    #Synthesize a sample sequence from the current model every 10000 update steps\n",
    "                    if(update_step % 10000 == 0):\n",
    "                        #Save to dataframe\n",
    "                        row = pd.DataFrame([[epoch, update_step, e, smooth_loss,loss]], columns=['epoch','update_step','e','smooth_loss','loss'])\n",
    "                        df = pd.concat([df,row], axis=0, ignore_index=True)\n",
    "                        #t.set_description(f'Smooth loss <{smooth_loss}>')\n",
    "                        # Print the stats continuously to a file\n",
    "                        Y = self.synthesize(hprev, X[:,0], 200)\n",
    "                        with open('stats.txt', 'a') as f:\n",
    "                            #Start by a seperator\n",
    "                            f.write(\"\\n\\n\")\n",
    "                            #Write epoch\n",
    "                            f.write(f\"Epoch: {epoch}\\n\")\n",
    "                            f.write(f\"Update Step: {update_step}\\n\")\n",
    "                            f.write(f\"Smooth Loss: {smooth_loss}\\n\")\n",
    "                            string = self.convert_to_character_sequence(Y)\n",
    "                            print(\"Update step: \", update_step)\n",
    "                            print(string)\n",
    "                            f.write(f\"Sample Sequence: {string}\\n\")\n",
    "                    update_step +=1\n",
    "            smooth_loss_epoch.append(smooth_loss_list)\n",
    "            \n",
    "        return self.params,best_model, smooth_loss,smooth_loss_epoch,df\n",
    "    # Function to generate a sequence of characters using the RNN model\n",
    "    def synthesize(self, h0, x0, n):\n",
    "        \"\"\"\n",
    "        Synthesize a sequence of characters using the RNN model.\n",
    "        Inputs:\n",
    "            h0: Initial hidden state\n",
    "            x0: Initial input vector  (one hot encoded) (K x 1)\n",
    "            n: Number of characters to synthesize\n",
    "        Outputs:\n",
    "            Y: One hot encoded sequence of characters (n x K)\n",
    "        \"\"\"\n",
    "\n",
    "        Y = np.zeros((self.K, n))\n",
    "        h = h0\n",
    "        x = x0\n",
    "        for t in range(n):\n",
    "            \n",
    "            a,h,o,p = self.forward_step(h, x)\n",
    "\n",
    "            # Randomly select the next character from our probability p\n",
    "            \n",
    "            index = np.random.choice(range(self.K), p = p.ravel())\n",
    "            x = np.zeros((self.K))# np.eye(self.K)[index]\n",
    "            x[index] = 1\n",
    "            # One hot encode the character chosen, set row index to 1\n",
    "            Y[:,t] = np.eye(self.K)[index]\n",
    "        return Y\n",
    "    \n",
    "    def convert_to_character_sequence(self,Y):\n",
    "        \"\"\"\n",
    "        Convert a sequence of one hot encoded characters to a sequence of characters.\n",
    "        Inputs:\n",
    "            Y: One hot encoded sequence of characters (n x K)\n",
    "        Outputs:\n",
    "            sequence: Sequence of characters\n",
    "        \"\"\"\n",
    "        sequence = \"\"\n",
    "        for i in range(Y.shape[1]):\n",
    "            sequence += self.ind_to_char[np.argmax(Y[:,i])]\n",
    "        return sequence\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        \"\"\"\n",
    "        Set the parameters of the model.\n",
    "        Inputs:\n",
    "            params: Dictionary containing the parameters\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "    def compute_grads_num_slow(self,X,Y,h=1e-5):\n",
    "        # Numerical gradient calculation using finite difference\n",
    "        # Slow implementation\n",
    "        # Initialize gradients\n",
    "        gradients = {}\n",
    "        for key in self.params:\n",
    "            gradients['d'+key] = np.zeros(self.params[key].shape)\n",
    "        \n",
    "        # Set hprev to zero\n",
    "        hprev = np.zeros((self.m,1))\n",
    "        for key in self.params:\n",
    "            for i in tqdm(range(self.params[key].size)):\n",
    "                \n",
    "                old = self.params[key].flat[i] # Store the original value of the parameter\n",
    "                self.params[key].flat[i] =  old - h\n",
    "                l1 = self.compute_loss(X,Y,hprev)\n",
    "                self.params[key].flat[i] = old + h\n",
    "                l2 = self.compute_loss(X,Y,hprev)\n",
    "                gradients['d'+key].flat[i] = (l2 - l1)/(2*h)\n",
    "                self.params[key].flat[i] = old # Reset the value of the parameter\n",
    "        return gradients\n",
    "    \n",
    "    def test_gradients(self, X, Y):\n",
    "        \"\"\"\n",
    "        Test the gradients computed using the function compute_gradients\n",
    "        Inputs:\n",
    "            X: Input sequence of characters\n",
    "            Y: Output sequence of characters\n",
    "            hprev: Initial hidden state\n",
    "        \"\"\"\n",
    "        hprev = np.zeros((self.m,1))\n",
    "        \n",
    "        # Forward pass\n",
    "        P, forward = self.forward_pass(hprev, X, Y)\n",
    "        # Compute gradients using backprop\n",
    "        gradients_a = self.compute_gradients(X, Y, P)\n",
    "        # Compute gradients using numerical gradient\n",
    "        gradients_n = self.compute_grads_num_slow(X, Y,h=1e-4)\n",
    "\n",
    "        df = pd.DataFrame(columns=['gradient','absolute error ok (%)','absolute worst diff','relative error ok (%)', 'relative worst diff','average relative error'])\n",
    "\n",
    "        threshold = 1e-5\n",
    "        # Compute the relative error\n",
    "        for key in gradients_a:\n",
    "            a = gradients_a[key]\n",
    "            n = gradients_n[key]\n",
    "            # Check if any NaN values seperately for numerical and analytical gradients\n",
    "            if np.isnan(a).any():\n",
    "                print(\"!! Analytical gradient has NaN values\")\n",
    "            if np.isnan(n).any():\n",
    "                print(\"!! Numerical gradient has NaN values\")\n",
    "\n",
    "            # Compute the absolute and relative error\n",
    "            abs_error = np.abs(a-n)\n",
    "            # Compute how many datapoints are below the threshold as an accuracy measure\n",
    "            abs_acc = np.mean(abs_error < threshold)*100\n",
    "            abs_worst = np.max(abs_error)\n",
    "            #Same as above but for relative error\n",
    "            rel_error = np.abs(a-n)/np.maximum(1e-6,np.abs(a)+np.abs(n))\n",
    "            rel_acc = np.mean(rel_error < threshold)*100\n",
    "            rel_worst = np.max(rel_error)\n",
    "            #Average relative error\n",
    "            avg_rel_error = np.mean(rel_error)\n",
    "\n",
    "            row = pd.DataFrame([{'gradient':key,'absolute error ok (%)':abs_acc,'absolute worst diff': abs_worst,'relative error ok (%)':rel_acc,'relative worst diff':rel_worst,'average relative error':avg_rel_error}])\n",
    "            df = pd.concat([df,row], axis=0, ignore_index=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "seq_length = 25\n",
    "m = 100\n",
    "\n",
    "# Initialize the hidden state to zero\n",
    "h0 = np.zeros((m, 1))\n",
    "\n",
    "book_data,book_chars, char_to_ind, ind_to_char,K = process_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, K, char_to_ind, ind_to_char)\n",
    "# Train the RNN model and get the trained parameters\n",
    "params,best_model, smooth_loss,smooth_loss_epochs,stats = rnn.train(book_data, h0, eta, 10, seq_length,optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesize text using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:  12.1730772637276\n",
      "\n",
      "\"An, said. Cislipidate asofe.\n",
      "\"Now.\n",
      "\"We've in undonted,\" croble ot shitaset; she wand was beenly hass comesnded task she lingonited sotht you?\" .\"\n",
      "\"Whot Harry.\n",
      "\"Khtichr scraars giving as Ron, his wancer.  Ouldluble,\" shanrinary, Hond, a bullared knowly, Deantant more.  See I you, face unice,\" saw Bancy was glince, at the Snckoose, and their werudleds make here a cercy was looked conjurrighe; to mu. They,\" said Kn his the snirting yeljom.  Ibach what said, leach mack Drigh.\n",
      "That sudged a prown. \"Peogly, caumblac lakusce, Stael \"He'd if then hirspingt keent.\n",
      "\"Salk sharl and undely ancelfe me Aice, oven fate as have Bar!\" searighit lober to for treet.\n",
      "\"A sermione.\"\n",
      "FYou'velosed eckine?\" sere she wall spoolly one itch macinjt ouch ank Moody's ale to with Cris, said, Summanand when ho now an off hlet, villy?\"\n",
      "\"Krumert-insisby Be to brignony know, a it mime squorking to be,\" said Mr.\n",
      "\"Maged witch arry these I figy,\" said Kritch his moms?\" said How uselved cloully, sit had fraster.\" \"Velved \n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(m, K, char_to_ind, ind_to_char)\n",
    "rnn.set_params(best_model['params'])\n",
    "\n",
    "loss = best_model['loss']\n",
    "print(\"Best loss: \",loss)\n",
    "# Generate a sequence of characters\n",
    "h0 = np.zeros((m, 1))\n",
    "x0 = one_hot(char_to_ind['T'],K)\n",
    "Y = rnn.synthesize(h0, x0, 1000)\n",
    "s = rnn.convert_to_character_sequence(Y)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result after 10 epochs of training\n",
    "\n",
    "Best loss: 12.1730772637276\n",
    "\n",
    "\n",
    "_\"An, said. Cislipidate asofe.\n",
    "\"Now.\n",
    "\"We've in undonted,\" croble ot shitaset; she wand was beenly hass comesnded task she lingonited sotht you?\" .\"\n",
    "\"Whot Harry.\n",
    "\"Khtichr scraars giving as Ron, his wancer.  Ouldluble,\" shanrinary, Hond, a bullared knowly, Deantant more.  See I you, face unice,\" saw Bancy was glince, at the Snckoose, and their werudleds make here a cercy was looked conjurrighe; to mu. They,\" said Kn his the snirting yeljom.  Ibach what said, leach mack Drigh.\n",
    "That sudged a prown. \"Peogly, caumblac lakusce, Stael \"He'd if then hirspingt keent.\n",
    "\"Salk sharl and undely ancelfe me Aice, oven fate as have Bar!\" searighit lober to for treet.\n",
    "\"A sermione.\"\n",
    "FYou'velosed eckine?\" sere she wall spoolly one itch macinjt ouch ank Moody's ale to with Cris, said, Summanand when ho now an off hlet, villy?\"\n",
    "\"Krumert-insisby Be to brignony know, a it mime squorking to be,\" said Mr.\n",
    "\"Maged witch arry these I figy,\" said Kritch his moms?\" said How uselved cloully, sit had fraster.\" \"Velved_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
